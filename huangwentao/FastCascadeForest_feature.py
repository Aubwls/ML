# coding=utf-8
import time
import pandas as pd
import numpy as np
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import KFold, cross_val_score, GridSearchCV
import warnings
from scipy import stats

warnings.filterwarnings('ignore')

start_time = time.time()

# åŠ è½½æ•°æ®é›†
train_dataSet = pd.read_csv(r'modified_æ•°æ®é›†Time_Series661_detail.dat')
test_dataSet = pd.read_csv(r'modified_æ•°æ®é›†Time_Series662_detail.dat')

columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']
noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr', 'Error_H2O_density',
                 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train_raw = train_dataSet[noise_columns]
y_train = train_dataSet[columns]
X_test_raw = test_dataSet[noise_columns]
y_test = test_dataSet[columns]

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataSet)}, æµ‹è¯•é›†å¤§å°: {len(test_dataSet)}")
print(f"è¾“å…¥ç‰¹å¾æ•°: {len(noise_columns)}, è¾“å‡ºå˜é‡æ•°: {len(columns)}")

# 1. æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†
print("\n=== æ•°æ®é¢„å¤„ç† ===")


# å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†ï¼ˆé’ˆå¯¹CO2ç›¸å…³å˜é‡ï¼‰
def handle_outliers(data, columns, z_threshold=3):
    """ä½¿ç”¨Z-scoreæ–¹æ³•å¤„ç†å¼‚å¸¸å€¼"""
    data_clean = data.copy()
    outlier_count = 0

    for col in columns:
        z_scores = np.abs(stats.zscore(data[col]))
        outliers = z_scores > z_threshold
        outlier_count += outliers.sum()

        if outliers.any():
            # ä½¿ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼
            median_val = data[col].median()
            data_clean.loc[outliers, col] = median_val
            print(f"  {col}: æ£€æµ‹åˆ°{outliers.sum()}ä¸ªå¼‚å¸¸å€¼ï¼Œç”¨ä¸­ä½æ•°{median_val:.4f}æ›¿æ¢")

    return data_clean, outlier_count


# å¤„ç†è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼ï¼ˆç‰¹åˆ«æ˜¯CO2ç›¸å…³å˜é‡ï¼‰
print("å¤„ç†å¼‚å¸¸å€¼...")
train_data_clean, train_outliers = handle_outliers(y_train, ['CO2_density', 'CO2_density_fast_tmpr'])
y_train_clean = train_data_clean[columns]

# æ ‡å‡†åŒ– - ä½¿ç”¨RobustScalerå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
scaler_X = RobustScaler()
scaler_y = RobustScaler()

X_train_scaled = scaler_X.fit_transform(X_train_raw)
X_test_scaled = scaler_X.transform(X_test_raw)

y_train_scaled = scaler_y.fit_transform(y_train_clean)
y_test_scaled = scaler_y.transform(y_test)

print(f"å¤„ç†äº† {train_outliers} ä¸ªå¼‚å¸¸å€¼")
print("ä½¿ç”¨RobustScalerè¿›è¡Œæ•°æ®æ ‡å‡†åŒ–å®Œæˆ")


# 2. æ”¹è¿›çš„çº§è”éšæœºæ£®æ—å®ç°
class OptimizedCascadeRandomForest:
    def __init__(self, n_levels=3, n_estimators=100, max_depth=None, min_samples_split=10,
                 min_samples_leaf=5, max_features='auto', early_stopping_rounds=2,
                 validation_split=0.1, random_state=42):
        """
        æ”¹è¿›çš„çº§è”éšæœºæ£®æ—æ¨¡å‹

        Args:
            n_levels: æœ€å¤§çº§è”å±‚æ•°
            n_estimators: æ¯å±‚éšæœºæ£®æ—çš„æ ‘æ•°é‡
            max_depth: æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
            min_samples_split: åˆ†è£‚èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
            min_samples_leaf: å¶èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
            max_features: å¯»æ‰¾æœ€ä½³åˆ†å‰²æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°é‡
            early_stopping_rounds: æ—©åœè½®æ•°
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
        """
        self.n_levels = n_levels
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.early_stopping_rounds = early_stopping_rounds
        self.validation_split = validation_split
        self.random_state = random_state
        self.models = []  # å­˜å‚¨æ¯å±‚çš„æ¨¡å‹
        self.feature_importances = []  # å­˜å‚¨æ¯å±‚çš„ç‰¹å¾é‡è¦æ€§
        self.best_level = 0  # æœ€ä½³å±‚æ•°ï¼ˆæ—©åœï¼‰

    def _create_train_val_split(self, X, y):
        """åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†"""
        n_samples = X.shape[0]
        n_val = int(n_samples * self.validation_split)

        indices = np.arange(n_samples)
        np.random.seed(self.random_state)
        np.random.shuffle(indices)

        val_indices = indices[:n_val]
        train_indices = indices[n_val:]

        X_train, X_val = X[train_indices], X[val_indices]
        y_train, y_val = y[train_indices], y[val_indices]

        return X_train, X_val, y_train, y_val

    def fit(self, X, y):
        """
        è®­ç»ƒæ”¹è¿›çš„çº§è”éšæœºæ£®æ—æ¨¡å‹

        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å˜é‡
        """
        print(f"è®­ç»ƒä¼˜åŒ–çº§è”éšæœºæ£®æ— (æœ€å¤§å±‚æ•°={self.n_levels})...")

        # åˆå§‹åŒ–ç‰¹å¾
        current_features = X.copy()
        best_val_score = -np.inf
        no_improvement_count = 0

        for level in range(self.n_levels):
            print(f"\nè®­ç»ƒç¬¬ {level + 1} å±‚:")

            # åˆ›å»ºè®­ç»ƒéªŒè¯é›†
            X_train, X_val, y_train, y_val = self._create_train_val_split(
                current_features, y
            )

            # ä¸ºæ¯ä¸ªç›®æ ‡å˜é‡è®­ç»ƒä¸€ä¸ªéšæœºæ£®æ—
            level_models = []
            level_importances = []
            level_val_scores = []

            for i in range(y.shape[1]):
                # åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹
                rf = RandomForestRegressor(
                    n_estimators=self.n_estimators,
                    max_depth=self.max_depth,
                    min_samples_split=self.min_samples_split,
                    min_samples_leaf=self.min_samples_leaf,
                    max_features=self.max_features,
                    bootstrap=True,
                    random_state=self.random_state + level * 100 + i,
                    n_jobs=-1
                )

                # è®­ç»ƒæ¨¡å‹
                rf.fit(X_train, y_train[:, i])
                level_models.append(rf)
                level_importances.append(rf.feature_importances_)

                # åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°
                y_pred_val = rf.predict(X_val)
                r2_val = r2_score(y_val[:, i], y_pred_val)
                level_val_scores.append(r2_val)

                # åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°
                y_pred_train = rf.predict(X_train)
                r2_train = r2_score(y_train[:, i], y_pred_train)
                print(f"  ç›®æ ‡å˜é‡ {i + 1}: è®­ç»ƒRÂ² = {r2_train:.4f}, éªŒè¯RÂ² = {r2_val:.4f}")

            # è®¡ç®—å¹³å‡éªŒè¯åˆ†æ•°
            avg_val_score = np.mean(level_val_scores)
            print(f"  å¹³å‡éªŒè¯RÂ²: {avg_val_score:.4f}")

            # ä¿å­˜å½“å‰å±‚çš„æ¨¡å‹å’Œç‰¹å¾é‡è¦æ€§
            self.models.append(level_models)
            self.feature_importances.append(level_importances)

            # æ—©åœæ£€æŸ¥
            if avg_val_score > best_val_score:
                best_val_score = avg_val_score
                self.best_level = level + 1
                no_improvement_count = 0
                print(f"  âœ… æ€§èƒ½æå‡ï¼Œæœ€ä½³å±‚æ•°æ›´æ–°ä¸º {self.best_level}")
            else:
                no_improvement_count += 1
                print(f"  âš ï¸ æ€§èƒ½æœªæå‡ ({no_improvement_count}/{self.early_stopping_rounds})")

                if no_improvement_count >= self.early_stopping_rounds:
                    print(f"  ğŸ›‘ æ—©åœè§¦å‘ï¼Œæœ€ç»ˆä½¿ç”¨ {self.best_level} å±‚")
                    break

            # å¦‚æœä¸æ˜¯æœ€åä¸€å±‚ï¼Œåˆ™ç”Ÿæˆæ–°çš„ç‰¹å¾ç”¨äºä¸‹ä¸€å±‚
            if level < self.n_levels - 1:
                # ä½¿ç”¨å½“å‰å±‚çš„é¢„æµ‹ä½œä¸ºæ–°ç‰¹å¾
                new_features = []
                for i, model in enumerate(level_models):
                    pred = model.predict(current_features).reshape(-1, 1)
                    new_features.append(pred)

                # å°†åŸå§‹ç‰¹å¾å’Œé¢„æµ‹ç‰¹å¾åˆå¹¶
                new_features = np.hstack(new_features)
                combined_features = np.hstack([current_features, new_features])

                print(
                    f"  ç”Ÿæˆæ–°ç‰¹å¾: åŸç‰¹å¾{current_features.shape[1]} + é¢„æµ‹ç‰¹å¾{new_features.shape[1]} = æ€»ç‰¹å¾{combined_features.shape[1]}")

                # æ›´æ–°å½“å‰ç‰¹å¾ä¸ºç»„åˆç‰¹å¾
                current_features = combined_features

        print(f"\næœ€ç»ˆæ¨¡å‹: {len(self.models)}å±‚ (æœ€ä½³{self.best_level}å±‚)")

    def predict(self, X):
        """
        ä½¿ç”¨çº§è”éšæœºæ£®æ—è¿›è¡Œé¢„æµ‹

        Args:
            X: è¾“å…¥ç‰¹å¾

        Returns:
            é¢„æµ‹ç»“æœ
        """
        current_features = X.copy()

        # åªä½¿ç”¨æœ€ä½³å±‚æ•°è¿›è¡Œé¢„æµ‹
        for level, level_models in enumerate(self.models[:self.best_level]):
            if level < self.best_level - 1:
                # å¯¹äºä¸­é—´å±‚ï¼Œç”Ÿæˆæ–°ç‰¹å¾
                new_features = []
                for model in level_models:
                    pred = model.predict(current_features).reshape(-1, 1)
                    new_features.append(pred)

                new_features = np.hstack(new_features)
                current_features = np.hstack([current_features, new_features])
            else:
                # å¯¹äºæœ€åä¸€å±‚ï¼Œç›´æ¥è¿›è¡Œé¢„æµ‹
                predictions = []
                for i, model in enumerate(level_models):
                    pred = model.predict(current_features)
                    predictions.append(pred)

                return np.column_stack(predictions)

        return None


# 3. è®­ç»ƒä¼˜åŒ–åçš„çº§è”éšæœºæ£®æ—æ¨¡å‹
print("\n=== è®­ç»ƒä¼˜åŒ–çº§è”éšæœºæ£®æ—æ¨¡å‹ ===")

# ä¸ºç›®æ ‡å˜é‡åˆ›å»ºä¼˜åŒ–çº§è”æ¨¡å‹
cascade_models = {}
y_predict_scaled = np.zeros_like(y_test_scaled)
all_train_scores = []
all_val_scores = []
all_test_scores = []

# ä¸ºæ¯ä¸ªç›®æ ‡å˜é‡è®­ç»ƒå•ç‹¬çš„ä¼˜åŒ–çº§è”æ¨¡å‹
for i, target_name in enumerate(columns):
    print(f"\n{'=' * 60}")
    print(f"è®­ç»ƒç›®æ ‡å˜é‡ {i + 1}/{len(columns)}: {target_name}")
    print('=' * 60)

    # æå–å½“å‰ç›®æ ‡å˜é‡
    y_train_target = y_train_scaled[:, i].reshape(-1, 1)

    # æ ¹æ®ç›®æ ‡å˜é‡è°ƒæ•´ä¼˜åŒ–å‚æ•°
    if target_name in ['T_SONIC', 'H2O_density']:
        # å¯¹è¿‡æ‹Ÿåˆä¸¥é‡çš„å˜é‡ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
        n_levels = 5
        n_estimators = 80
        max_depth = 6
        min_samples_split = 15
        min_samples_leaf = 8
        max_features = 0.7  # é™åˆ¶ç‰¹å¾ä½¿ç”¨æ¯”ä¾‹
        early_stopping_rounds = 2

    elif target_name in ['CO2_density', 'CO2_density_fast_tmpr']:
        # å¯¹è¿™äº›å˜é‡ä½¿ç”¨ä¸­ç­‰é…ç½®ï¼Œæ³¨æ„é˜²æ­¢è¿‡æ‹Ÿåˆ
        n_levels = 4
        n_estimators = 100
        max_depth = 8
        min_samples_split = 10
        min_samples_leaf = 5
        max_features = 'sqrt'
        early_stopping_rounds = 2

    else:
        # å¯¹ä¿¡å·å¼ºåº¦å˜é‡ä½¿ç”¨è¾ƒæµ…çš„çº§è”
        n_levels = 3
        n_estimators = 60
        max_depth = 5
        min_samples_split = 8
        min_samples_leaf = 4
        max_features = 0.8
        early_stopping_rounds = 2

    # åˆ›å»ºä¼˜åŒ–çº§è”æ¨¡å‹
    cascade_model = OptimizedCascadeRandomForest(
        n_levels=n_levels,
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        early_stopping_rounds=early_stopping_rounds,
        validation_split=0.15,  # 15%éªŒè¯é›†
        random_state=42 + i * 10
    )

    # è®­ç»ƒæ¨¡å‹
    cascade_model.fit(X_train_scaled, y_train_target)
    cascade_models[target_name] = cascade_model

    # åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°ï¼ˆä½¿ç”¨æœ€ä½³å±‚æ•°ï¼‰
    y_train_pred = cascade_model.predict(X_train_scaled)
    train_r2 = r2_score(y_train_target, y_train_pred)
    all_train_scores.append(train_r2)

    # åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°
    val_r2 = cascade_model.best_val_score if hasattr(cascade_model, 'best_val_score') else train_r2
    all_val_scores.append(val_r2)

    # åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°
    y_test_pred = cascade_model.predict(X_test_scaled)
    y_predict_scaled[:, i] = y_test_pred.flatten()
    test_r2 = r2_score(y_test_scaled[:, i], y_test_pred.flatten())
    all_test_scores.append(test_r2)

    print(f"\nğŸ“Š {target_name} ç»“æœæ±‡æ€»:")
    print(f"  é…ç½®: æœ€å¤§å±‚æ•°={n_levels}, æœ€ä½³å±‚æ•°={cascade_model.best_level}")
    print(f"  æ ‘æ•°é‡={n_estimators}, æ·±åº¦={max_depth}")
    print(f"  è®­ç»ƒé›†RÂ²: {train_r2:.4f}, éªŒè¯é›†RÂ²: {val_r2:.4f}, æµ‹è¯•é›†RÂ²: {test_r2:.4f}")

    # è¿‡æ‹Ÿåˆåˆ†æ
    overfit = train_r2 - test_r2
    val_overfit = train_r2 - val_r2

    if val_overfit > 0.15:
        print(f"  âš ï¸ éªŒè¯é›†ä¸¥é‡è¿‡æ‹Ÿåˆ: å·®å¼‚={val_overfit:.4f}")
    elif val_overfit > 0.08:
        print(f"  âš ï¸ éªŒè¯é›†ä¸­åº¦è¿‡æ‹Ÿåˆ: å·®å¼‚={val_overfit:.4f}")
    elif val_overfit > 0.04:
        print(f"  âš ï¸ éªŒè¯é›†è½»å¾®è¿‡æ‹Ÿåˆ: å·®å¼‚={val_overfit:.4f}")
    else:
        print(f"  âœ… éªŒè¯é›†æ‹Ÿåˆè‰¯å¥½: å·®å¼‚={val_overfit:.4f}")

# 4. åå‘æ ‡å‡†åŒ–
print("\n=== åå‘æ ‡å‡†åŒ–é¢„æµ‹ç»“æœ ===")
y_predict = scaler_y.inverse_transform(y_predict_scaled)

# 5. ä¿å­˜ç»“æœ
results = []
for true_value, pred_value in zip(y_test.values, y_predict):
    error = np.abs(true_value - pred_value)
    formatted_true = ' '.join(f"{x:.6f}" for x in true_value)
    formatted_pred = ' '.join(f"{x:.6f}" for x in pred_value)
    formatted_error = ' '.join(f"{x:.6f}" for x in error)
    results.append([formatted_true, formatted_pred, formatted_error])

result_df = pd.DataFrame(results, columns=['True_Value', 'Predicted_Value', 'Error'])
result_df.to_csv("result_OptimizedCascadeRF.csv", index=False)
print("ç»“æœå·²ä¿å­˜åˆ°: result_OptimizedCascadeRF.csv")

# 6. æ€§èƒ½è¯„ä¼°
print("\n=== è¯¦ç»†æ€§èƒ½è¯„ä¼° ===")
performance_metrics = []
mae_values = []
mse_values = []

for i, column in enumerate(columns):
    y_pred_original = y_predict[:, i]
    y_true_original = y_test.iloc[:, i]

    r2 = r2_score(y_true_original, y_pred_original)
    rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))
    mae = np.mean(np.abs(y_true_original - y_pred_original))

    mae_values.append(mae)
    mse_values.append(rmse ** 2)

    # è®¡ç®—ç›¸å¯¹è¯¯å·®
    y_mean = y_true_original.mean()
    y_std = y_true_original.std()

    if abs(y_mean) > 1e-8:
        relative_rmse = rmse / abs(y_mean)
        relative_mae = mae / abs(y_mean)
    else:
        relative_rmse = rmse
        relative_mae = mae

    # è®¡ç®—è¯¯å·®åœ¨æ ‡å‡†å·®ä¸­çš„æ¯”ä¾‹
    error_std_ratio = mae / y_std if y_std > 1e-8 else mae

    performance_metrics.append([
        column, all_train_scores[i], all_val_scores[i], all_test_scores[i], r2,
        rmse, mae, relative_rmse, relative_mae, error_std_ratio
    ])

    print(f"\n{column}:")
    print(f"  è®­ç»ƒRÂ²: {all_train_scores[i]:.4f}, éªŒè¯RÂ²: {all_val_scores[i]:.4f}, æµ‹è¯•RÂ²: {test_r2:.4f}")
    print(f"  RMSE: {rmse:.4f}, MAE: {mae:.4f}")
    print(f"  ç›¸å¯¹è¯¯å·®: RMSE={relative_rmse:.2%}, MAE={relative_mae:.2%}")
    print(f"  è¯¯å·®/æ ‡å‡†å·®: {error_std_ratio:.2%}")

# ä¿å­˜æ€§èƒ½æŒ‡æ ‡
perf_columns = ['Variable', 'Train_R2', 'Val_R2', 'Test_R2_scaled', 'Test_R2',
                'RMSE', 'MAE', 'Relative_RMSE', 'Relative_MAE', 'Error/Std_Ratio']
perf_df = pd.DataFrame(performance_metrics, columns=perf_columns)
perf_df.to_csv("performance_optimized_cascade.csv", index=False)

# 7. å…³é”®æŒ‡æ ‡è®¡ç®—
print("\n=== å…³é”®æŒ‡æ ‡æ±‡æ€» ===")
total_mae = np.mean(mae_values)
total_weighted_mae = np.average(mae_values, weights=[1.0, 0.5, 0.5, 1.0, 0.2, 0.2])  # åŠ æƒå¹³å‡
total_mse = np.mean(mse_values)
total_rmse = np.sqrt(total_mse)

print(f"ç®—æœ¯å¹³å‡MAE: {total_mae:.6f}")
print(f"åŠ æƒå¹³å‡MAE: {total_weighted_mae:.6f} (T_SONICå’ŒH2O_densityæƒé‡=1ï¼ŒCO2ç›¸å…³=0.5ï¼Œä¿¡å·å¼ºåº¦=0.2)")
print(f"æ€»RMSE: {total_rmse:.6f}")
print(f"å¹³å‡è®­ç»ƒRÂ²: {np.mean(all_train_scores):.4f}")
print(f"å¹³å‡éªŒè¯RÂ²: {np.mean(all_val_scores):.4f}")
print(f"å¹³å‡æµ‹è¯•RÂ²: {np.mean(all_test_scores):.4f}")

# 8. è¯¯å·®åˆ†å¸ƒåˆ†æï¼ˆè¿‡æ»¤æå¤§å¼‚å¸¸å€¼ï¼‰
print("\n=== è¯¯å·®åˆ†å¸ƒåˆ†æ (è¿‡æ»¤å¼‚å¸¸å€¼å) ===")
for i, col in enumerate(columns):
    errors = np.abs(y_test.iloc[:, i] - y_predict[:, i])

    # è¿‡æ»¤æ‰æå¤§è¯¯å·®ï¼ˆä»…æ˜¾ç¤º99%åˆ†ä½æ•°ä»¥å†…çš„æ•°æ®ï¼‰
    q99 = np.percentile(errors, 99)
    errors_filtered = errors[errors <= q99]

    print(f"{col}:")
    print(f"  æ ·æœ¬æ•°: {len(errors_filtered)}/{len(errors)} (è¿‡æ»¤äº†{len(errors) - len(errors_filtered)}ä¸ªå¼‚å¸¸è¯¯å·®)")
    print(f"  æœ€å°å€¼: {errors_filtered.min():.6f}")
    print(f"  25%åˆ†ä½æ•°: {np.percentile(errors_filtered, 25):.6f}")
    print(f"  ä¸­ä½æ•°: {np.median(errors_filtered):.6f}")
    print(f"  75%åˆ†ä½æ•°: {np.percentile(errors_filtered, 75):.6f}")
    print(f"  90%åˆ†ä½æ•°: {np.percentile(errors_filtered, 90):.6f}")
    print(f"  99%åˆ†ä½æ•°: {np.percentile(errors_filtered, 99):.6f}")
    print(f"  æœ€å¤§å€¼(è¿‡æ»¤å): {errors_filtered.max():.6f}")
    print(f"  åŸå§‹æœ€å¤§å€¼: {errors.max():.6f}")

# 9. ç›®æ ‡è¾¾æˆè¯„ä¼°
target_mae = 0.5
print(f"\n=== ç›®æ ‡è¯„ä¼° (ç›®æ ‡MAE < {target_mae}) ===")

variables_achieved = []
variables_not_achieved = []

for col, mae in zip(columns, mae_values):
    if mae < target_mae:
        variables_achieved.append((col, mae))
    else:
        variables_not_achieved.append((col, mae))

print(f"è¾¾åˆ°ç›®æ ‡çš„å˜é‡ ({len(variables_achieved)}/{len(columns)}):")
for col, mae in variables_achieved:
    print(f"  âœ… {col}: MAE={mae:.6f}")

print(f"æœªè¾¾åˆ°ç›®æ ‡çš„å˜é‡ ({len(variables_not_achieved)}/{len(columns)}):")
for col, mae in variables_not_achieved:
    improvement_needed = mae - target_mae
    percent_improvement = improvement_needed / mae * 100
    print(f"  âŒ {col}: MAE={mae:.6f} (éœ€è¦é™ä½{improvement_needed:.6f}, {percent_improvement:.1f}%)")

# 10. æ¨¡å‹è¯Šæ–­å’Œå»ºè®®
print("\n=== æ¨¡å‹è¯Šæ–­å’Œå»ºè®® ===")

if total_weighted_mae < target_mae:
    print("ğŸ‰ æ­å–œï¼åŠ æƒå¹³å‡MAEå·²ä½äºç›®æ ‡å€¼0.5ï¼")
else:
    print(f"å½“å‰åŠ æƒå¹³å‡MAEä¸º {total_weighted_mae:.6f}ï¼Œè·ç¦»ç›®æ ‡è¿˜æœ‰ {total_weighted_mae - target_mae:.6f} çš„å·®è·")

    # è®¡ç®—æ¯ä¸ªå˜é‡éœ€è¦çš„æ”¹è¿›æ¯”ä¾‹
    print(f"\nğŸ“ˆ å„å˜é‡æ”¹è¿›ä¼˜å…ˆçº§:")
    improvement_priority = []
    for i, col in enumerate(columns):
        current_mae = mae_values[i]
        if current_mae > target_mae:
            needed_improvement = (current_mae - target_mae) / current_mae * 100
            improvement_priority.append((col, needed_improvement, current_mae))

    # æŒ‰æ”¹è¿›æ¯”ä¾‹æ’åº
    improvement_priority.sort(key=lambda x: x[1], reverse=True)

    for col, needed_percent, current_mae in improvement_priority:
        print(f"  {col}: éœ€è¦æ”¹è¿›{needed_percent:.1f}% (ä»{current_mae:.6f}åˆ°{target_mae:.6f})")

# 11. ä¸‹ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥
print("\n=== ä¸‹ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ ===")
print("å¦‚æœMAEä»ç„¶ä¸ç†æƒ³ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹é«˜çº§ç­–ç•¥:")
print("1. ğŸ¯ é›†æˆæ–¹æ³•:")
print("   - ä½¿ç”¨XGBoostæˆ–LightGBMä»£æ›¿éƒ¨åˆ†éšæœºæ£®æ—")
print("   - å¯¹ä¸åŒå˜é‡çš„æ¨¡å‹è¿›è¡ŒåŠ æƒé›†æˆ")
print("2. ğŸ“Š ç‰¹å¾å·¥ç¨‹:")
print("   - åˆ›å»ºç‰¹å¾äº¤äº’é¡¹")
print("   - æ·»åŠ æ»åç‰¹å¾ï¼ˆæ—¶é—´åºåˆ—ç‰¹æ€§ï¼‰")
print("   - ä½¿ç”¨PCAè¿›è¡Œç‰¹å¾é™ç»´")
print("3. âš™ï¸ æ¨¡å‹è°ƒä¼˜:")
print("   - ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
print("   - å°è¯•ä¸åŒçš„çº§è”ç­–ç•¥ï¼ˆå¦‚æ®‹å·®è¿æ¥ï¼‰")
print("   - å®ç°è‡ªé€‚åº”çº§è”æ·±åº¦")
print("4. ğŸ§¹ æ•°æ®ä¼˜åŒ–:")
print("   - æ›´ç²¾ç»†çš„å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†")
print("   - è€ƒè™‘æ•°æ®åˆ†æ®µå»ºæ¨¡ï¼ˆä¸åŒèŒƒå›´ç”¨ä¸åŒæ¨¡å‹ï¼‰")
print("   - å¢åŠ è®­ç»ƒæ•°æ®æˆ–ä½¿ç”¨æ•°æ®å¢å¼º")

# 12. æ¨¡å‹å‚æ•°æ€»ç»“
print("\n=== æœ€ç»ˆæ¨¡å‹å‚æ•°æ€»ç»“ ===")
summary_data = []
for col in columns:
    model = cascade_models[col]
    summary_data.append([
        col, model.best_level, model.n_estimators, model.max_depth,
        model.min_samples_split, model.min_samples_leaf,
        f"{all_train_scores[columns.index(col)]:.4f}",
        f"{all_val_scores[columns.index(col)]:.4f}",
        f"{all_test_scores[columns.index(col)]:.4f}",
        f"{mae_values[columns.index(col)]:.6f}"
    ])

summary_df = pd.DataFrame(summary_data, columns=[
    'Variable', 'Best_Levels', 'N_Estimators', 'Max_Depth',
    'Min_Samples_Split', 'Min_Samples_Leaf',
    'Train_R2', 'Val_R2', 'Test_R2', 'MAE'
])
print(summary_df.to_string(index=False))

# 13. æœ€ç»ˆæ€»ç»“
print("\n" + "=" * 70)
print("ä¼˜åŒ–çº§è”éšæœºæ£®æ—æœ€ç»ˆæ€»ç»“")
print("=" * 70)
print(f"ğŸ“Š æ€»ä½“æ€§èƒ½:")
print(f"  - ç®—æœ¯å¹³å‡MAE: {total_mae:.6f}")
print(f"  - åŠ æƒå¹³å‡MAE: {total_weighted_mae:.6f}")
print(f"  - æ€»RMSE: {total_rmse:.6f}")
print(f"  - å¹³å‡æµ‹è¯•RÂ²: {np.mean(all_test_scores):.4f}")
print(f"  - è¾¾åˆ°ç›®æ ‡å˜é‡: {len(variables_achieved)}/{len(columns)}")

print(f"\nğŸ¯ ç›®æ ‡çŠ¶æ€: {'âœ… å·²è¾¾æˆ' if total_weighted_mae < target_mae else 'âŒ æœªè¾¾æˆ'}")
if total_weighted_mae < target_mae:
    print(f"   åŠ æƒå¹³å‡MAEæ¯”ç›®æ ‡ä½ {target_mae - total_weighted_mae:.6f}")
else:
    print(f"   éœ€è¦å†é™ä½ {total_weighted_mae - target_mae:.6f} æ‰èƒ½è¾¾åˆ°ç›®æ ‡")

print(f"\nğŸš€ ä¸»è¦æ”¹è¿›:")
print(f"  1. æ·»åŠ æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ")
print(f"  2. ä½¿ç”¨RobustScalerå¤„ç†å¼‚å¸¸å€¼")
print(f"  3. å¢åŠ éªŒè¯é›†è¿›è¡Œæ¨¡å‹é€‰æ‹©")
print(f"  4. ä¸ºä¸åŒå˜é‡å®šåˆ¶åŒ–å‚æ•°")
print(f"  5. è¿‡æ»¤æç«¯è¯¯å·®è¿›è¡Œåˆ†æ")

print(f"\nâ±ï¸  è¿è¡Œæ—¶é—´: {time.time() - start_time:.2f}ç§’")
print("=" * 70)