import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
import time
import warnings
import gc

warnings.filterwarnings('ignore')


class OptimizedCascadeForest:
    """
    ä¼˜åŒ–å‚æ•°è®¾ç½®çš„çº§è”æ£®æ—
    å¢åŠ å†…å­˜ç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–
    """

    def __init__(self, n_layers=2, n_estimators=80, random_state=217,
                 use_early_stopping=True, target_specific_params=None,
                 use_subsample=False, subsample_ratio=0.1):
        self.n_layers = n_layers
        self.n_estimators = n_estimators
        self.random_state = random_state
        self.use_early_stopping = use_early_stopping
        self.target_specific_params = target_specific_params or {}
        self.use_subsample = use_subsample
        self.subsample_ratio = subsample_ratio
        self.layers = []
        self.best_layer = 0

    def _create_optimized_forests(self, X, y, layer_idx, target_name):
        """åˆ›å»ºä¼˜åŒ–å‚æ•°è®¾ç½®çš„æ£®æ—"""
        forests = []

        # è·å–ç›®æ ‡ç‰¹å®šå‚æ•°
        target_params = self.target_specific_params.get(target_name, {})

        # å‡å°‘æ ‘çš„æ•°é‡å’Œæ·±åº¦ä»¥èŠ‚çœå†…å­˜
        n_est_for_layer = max(10, self.n_estimators // ((layer_idx + 1) * 2))
        max_depth_for_layer = min(15, target_params.get('max_depth', 15))

        # åŸºç¡€å‚æ•°é…ç½® - å‡å°‘å†…å­˜ä½¿ç”¨
        base_rf_params = {
            'n_estimators': n_est_for_layer,
            'max_depth': max_depth_for_layer,
            'min_samples_split': target_params.get('min_samples_split', 10),
            'min_samples_leaf': target_params.get('min_samples_leaf', 4),
            'max_features': target_params.get('max_features', 0.6),
            'bootstrap': True,
            'random_state': self.random_state + layer_idx * 100,
            'n_jobs': 1,
            'verbose': 0
        }

        base_et_params = {
            'n_estimators': n_est_for_layer,
            'max_depth': max_depth_for_layer,
            'min_samples_split': target_params.get('min_samples_split', 10),
            'min_samples_leaf': target_params.get('min_samples_leaf', 4),
            'max_features': target_params.get('max_features', 0.6),
            'bootstrap': False,
            'random_state': self.random_state + layer_idx * 100 + 50,
            'n_jobs': 1,
            'verbose': 0
        }

        # æ£®æ—é…ç½® - å‡å°‘æ£®æ—æ•°é‡ä»¥èŠ‚çœå†…å­˜
        forest_configs = [
            # é…ç½®1: æ ‡å‡†éšæœºæ£®æ—
            {
                'model': RandomForestRegressor,
                'params': base_rf_params
            },
            # é…ç½®2: æç«¯éšæœºæ ‘
            {
                'model': ExtraTreesRegressor,
                'params': base_et_params
            }
        ]

        # è®­ç»ƒæ‰€æœ‰æ£®æ—
        for config in forest_configs:
            try:
                model = config['model'](**config['params'])
                model.fit(X, y)
                forests.append(model)
                # æ¸…ç†å†…å­˜
                gc.collect()
            except Exception as e:
                print(f"  æ£®æ—è®­ç»ƒå¤±è´¥: {e}")
                continue

        return forests

    def fit(self, X, y, target_name):
        """è®­ç»ƒä¼˜åŒ–çº§è”æ£®æ—"""
        print(f"å¼€å§‹è®­ç»ƒ {target_name} çš„ä¼˜åŒ–çº§è”æ£®æ—...")
        start_fit = time.time()

        self.layers = []
        X_current = X.copy()
        best_score = -np.inf
        self.best_layer = 0

        # ä¿®æ”¹ï¼šç§»é™¤å±‚æ•°é™åˆ¶ï¼Œä½¿ç”¨self.n_layers
        for layer in range(self.n_layers):
            print(f"  è®­ç»ƒç¬¬ {layer + 1}/{self.n_layers} å±‚çº§è”...")

            # åˆ›å»ºä¼˜åŒ–æ£®æ—
            forests = self._create_optimized_forests(X_current, y, layer, target_name)

            if len(forests) == 0:
                print(f"  ç¬¬ {layer + 1} å±‚æ— æ³•åˆ›å»ºæ£®æ—ï¼Œè·³è¿‡...")
                continue

            self.layers.append(forests)

            # è¯„ä¼°å½“å‰å±‚æ€§èƒ½
            try:
                layer_predictions = []
                for forest in forests:
                    pred = forest.predict(X_current)
                    layer_predictions.append(pred.reshape(-1, 1))

                # è®¡ç®—å½“å‰å±‚çš„å¹³å‡é¢„æµ‹
                current_pred = np.mean(layer_predictions, axis=0).flatten()
                current_r2 = r2_score(y, current_pred)

                print(f"  ç¬¬ {layer + 1} å±‚ RÂ²: {current_r2:.6f}")

                # æ—©åœæ£€æŸ¥
                if self.use_early_stopping and current_r2 > best_score:
                    best_score = current_r2
                    self.best_layer = layer
                elif self.use_early_stopping and layer > 0:
                    improvement = current_r2 - best_score
                    if improvement < 0.0005:  # æ—©åœé˜ˆå€¼
                        print(f"  æ—©åœè§¦å‘åœ¨ç¬¬ {layer + 1} å±‚ï¼Œæ”¹è¿›ä»…ä¸º {improvement:.6f}")
                        break
            except Exception as e:
                print(f"  ç¬¬ {layer + 1} å±‚è¯„ä¼°å¤±è´¥: {e}")
                continue

            # å¦‚æœä¸æ˜¯æœ€åä¸€å±‚ï¼Œç”Ÿæˆå¢å¼ºç‰¹å¾
            # ä¿®æ”¹ï¼šä½¿ç”¨self.n_layers - 1ä»£æ›¿ç¡¬ç¼–ç çš„2
            if layer < self.n_layers - 1 and len(layer_predictions) > 0:
                enhanced_features = np.hstack(layer_predictions)
                X_current = np.hstack([X, enhanced_features])
                print(f"  çº§è”å±‚ {layer + 1} å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X_current.shape[1]}")

            # æ¸…ç†å†…å­˜
            del layer_predictions
            gc.collect()

        # æœ€ç»ˆé¢„æµ‹å™¨ - ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹
        if len(self.layers) == 0:
            print("  ä½¿ç”¨ç®€å•éšæœºæ£®æ—ä½œä¸ºåå¤‡")
            self.fallback_model = RandomForestRegressor(
                n_estimators=30,
                max_depth=10,
                random_state=self.random_state,
                n_jobs=1,
                verbose=0
            )
            self.fallback_model.fit(X, y)
            self.use_fallback = True
        else:
            print("  è®­ç»ƒæœ€ç»ˆé›†æˆé¢„æµ‹å™¨...")
            # è·å–ç›®æ ‡ç‰¹å®šå‚æ•°ç”¨äºæœ€ç»ˆé¢„æµ‹å™¨
            target_params = self.target_specific_params.get(target_name, {})
            self.final_estimator = RandomForestRegressor(
                n_estimators=max(20, self.n_estimators // 2),
                max_depth=min(15, target_params.get('max_depth', 15)),
                min_samples_split=target_params.get('min_samples_split', 10),
                min_samples_leaf=target_params.get('min_samples_leaf', 4),
                max_features=target_params.get('max_features', 0.6),
                random_state=self.random_state,
                n_jobs=1,
                verbose=0
            )
            self.final_estimator.fit(X_current, y)
            self.use_fallback = False

            # æ¸…ç†å†…å­˜
            del X_current
            gc.collect()

        end_fit = time.time()
        print(f"  {target_name} çº§è”æ£®æ—è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {end_fit - start_fit:.2f}ç§’")
        if not self.use_fallback:
            print(f"  æœ€ä½³å±‚: {self.best_layer + 1}")
            print(f"  æ€»å±‚æ•°: {len(self.layers)}")

        return self

    def predict(self, X):
        """ä½¿ç”¨ä¼˜åŒ–çº§è”æ£®æ—è¿›è¡Œé¢„æµ‹"""
        if hasattr(self, 'use_fallback') and self.use_fallback:
            return self.fallback_model.predict(X)

        X_current = X.copy()

        # åªä½¿ç”¨æœ€ä½³å±‚ä¹‹å‰çš„å±‚è¿›è¡Œç‰¹å¾å¢å¼º
        for layer_idx, forests in enumerate(self.layers):
            if layer_idx > self.best_layer:
                break

            predictions = []
            for forest in forests:
                pred = forest.predict(X_current)
                predictions.append(pred.reshape(-1, 1))

            if layer_idx < len(self.layers) - 1 and len(predictions) > 0:
                enhanced_features = np.hstack(predictions)
                X_current = np.hstack([X, enhanced_features])

        result = self.final_estimator.predict(X_current)

        # æ¸…ç†å†…å­˜
        del X_current
        gc.collect()

        return result


def cascade_forest_predict():
    """çº§è”æ£®æ—é¢„æµ‹æ–¹æ³• - ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    start_time = time.time()

    # åŠ è½½æ•°æ®
    try:
        train_dataSet = pd.read_csv(r'modified_æ•°æ®é›†Time_Series661_detail.dat')
        test_dataSet = pd.read_csv(r'modified_æ•°æ®é›†Time_Series662_detail.dat')
        print("æ•°æ®åŠ è½½æˆåŠŸ")
    except FileNotFoundError as e:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return

    # å®šä¹‰ç‰¹å¾å’Œç›®æ ‡
    noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr',
                     'Error_H2O_density', 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']
    columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density',
               'H2O_sig_strgth', 'CO2_sig_strgth']

    # ä½¿ç”¨éƒ¨åˆ†æ•°æ®ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    print("æ³¨æ„: ä½¿ç”¨éƒ¨åˆ†æ•°æ®è¿›è¡Œè®­ç»ƒä»¥èŠ‚çœå†…å­˜...")
    sample_fraction = 0.2  # ä½¿ç”¨20%çš„æ•°æ®

    # é‡‡æ ·æ•°æ®
    train_indices = np.random.choice(len(train_dataSet),
                                     size=int(len(train_dataSet) * sample_fraction),
                                     replace=False)
    test_indices = np.random.choice(len(test_dataSet),
                                    size=int(len(test_dataSet) * sample_fraction),
                                    replace=False)

    X_train = train_dataSet.loc[train_indices, noise_columns].values
    y_train = train_dataSet.loc[train_indices, columns].values
    X_test = test_dataSet.loc[test_indices, noise_columns].values
    y_test = test_dataSet.loc[test_indices, columns].values

    print(f"ä½¿ç”¨ {sample_fraction * 100:.0f}% æ•°æ®")
    print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    # æ•°æ®æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()

    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    y_train_scaled = scaler_y.fit_transform(y_train)

    print("\nå¼€å§‹ä¼˜åŒ–çº§è”æ£®æ—è®­ç»ƒ...")

    # ç›®æ ‡ç‰¹å®šçš„å‚æ•°é…ç½® - å–æ¶ˆå±‚æ•°é™åˆ¶
    target_specific_params = {
        'T_SONIC': {
            'n_layers': 10,  # è®¾ç½®6å±‚
            'n_estimators': 50,  # å‡å°‘æ ‘çš„æ•°é‡
            'max_depth': 20,  # å‡å°‘æ·±åº¦
            'min_samples_split': 10,
            'min_samples_leaf': 4,
            'max_features': 0.6
        },
        'CO2_density': {
            'n_layers': 8,
            'n_estimators': 50,
            'max_depth': 20,
            'min_samples_split': 10,
            'min_samples_leaf': 4,
            'max_features': 0.6
        },
        'CO2_density_fast_tmpr': {
            'n_layers': 8,
            'n_estimators': 50,
            'max_depth': 20,
            'min_samples_split': 10,
            'min_samples_leaf': 4,
            'max_features': 0.6
        },
        'H2O_density': {
            'n_layers': 8,
            'n_estimators': 50,
            'max_depth': 15,
            'min_samples_split': 10,
            'min_samples_leaf': 4,
            'max_features': 0.6
        },
        'H2O_sig_strgth': {
            'n_layers': 8,
            'n_estimators': 50,
            'max_depth': 15,
            'min_samples_split': 15,
            'min_samples_leaf': 6,
            'max_features': 0.5
        },
        'CO2_sig_strgth': {
            'n_layers': 8,
            'n_estimators': 50,
            'max_depth': 15,
            'min_samples_split': 15,
            'min_samples_leaf': 6,
            'max_features': 0.5
        }
    }

    # ä¸ºæ¯ä¸ªç›®æ ‡è®­ç»ƒä¼˜åŒ–çº§è”æ£®æ—
    cascade_predictions = []
    cascade_models = {}

    print("\n" + "=" * 60)
    print("ä¼˜åŒ–çº§è”æ£®æ—è®­ç»ƒ (æ”¯æŒå¤šå±‚æ¶æ„)")
    print("=" * 60)

    for target_idx, target_name in enumerate(columns):
        print(f"\nè®­ç»ƒç›®æ ‡å˜é‡ {target_idx + 1}/{len(columns)}: {target_name}")

        # è·å–ç›®æ ‡ç‰¹å®šé…ç½®
        target_config = target_specific_params[target_name]

        # åˆ›å»ºä¼˜åŒ–çº§è”æ£®æ—
        cascade_model = OptimizedCascadeForest(
            n_layers=target_config['n_layers'],
            n_estimators=target_config['n_estimators'],
            random_state=217 + target_idx,
            use_early_stopping=True,
            target_specific_params=target_specific_params
        )

        # è®­ç»ƒæ¨¡å‹
        cascade_model.fit(X_train_scaled, y_train_scaled[:, target_idx], target_name)

        # é¢„æµ‹
        pred_scaled = cascade_model.predict(X_test_scaled)
        cascade_predictions.append(pred_scaled)
        cascade_models[target_name] = cascade_model

        # ç«‹å³è¯„ä¼°
        pred_temp = scaler_y.inverse_transform(
            np.column_stack([pred_scaled] * len(columns))
        )[:, target_idx]
        mae = np.mean(np.abs(y_test[:, target_idx] - pred_temp))
        print(f"  {target_name} æµ‹è¯•MAE: {mae:.4f}")

        # æ¸…ç†å†…å­˜
        gc.collect()

    # åˆå¹¶é¢„æµ‹ç»“æœ
    y_cascade_scaled = np.column_stack(cascade_predictions)
    y_cascade = scaler_y.inverse_transform(y_cascade_scaled)

    # æ€§èƒ½è¯„ä¼°
    print("\n" + "=" * 60)
    print("çº§è”æ£®æ—æ€§èƒ½è¯„ä¼°")
    print("=" * 60)

    mse = mean_squared_error(y_test, y_cascade)
    r2 = r2_score(y_test, y_cascade)
    mae = np.mean(np.abs(y_test - y_cascade))

    print(f"MSE: {mse:.6f}")
    print(f"RÂ²: {r2:.6f}")
    print(f"MAE: {mae:.6f}")

    # è¯¦ç»†åˆ†æ
    print("\n" + "=" * 60)
    print("çº§è”æ£®æ—è¯¦ç»†åˆ†æ")
    print("=" * 60)

    mae_per_column = np.mean(np.abs(y_test - y_cascade), axis=0)
    mse_per_column = np.mean((y_test - y_cascade) ** 2, axis=0)
    r2_per_column = [r2_score(y_test[:, i], y_cascade[:, i]) for i in range(len(columns))]

    for i, col in enumerate(columns):
        print(f"{col:>25}: MAE = {mae_per_column[i]:.4f}, MSE = {mse_per_column[i]:.4f}, RÂ² = {r2_per_column[i]:.4f}")

    avg_mae = np.mean(mae_per_column)
    avg_mse = np.mean(mse_per_column)
    avg_r2 = np.mean(r2_per_column)

    print(f"\nå¹³å‡MAE: {avg_mae:.4f}")
    print(f"å¹³å‡MSE: {avg_mse:.4f}")
    print(f"å¹³å‡RÂ²: {avg_r2:.4f}")

    # ä¿å­˜é¢„æµ‹ç»“æœ
    results_final = []
    for True_Value, Predicted_Value in zip(y_test, y_cascade):
        error = np.abs(True_Value - Predicted_Value)
        formatted_true_value = ' '.join(map(str, True_Value))
        formatted_predicted_value = ' '.join(map(str, Predicted_Value))
        formatted_error = ' '.join(map(str, error))
        results_final.append([formatted_true_value, formatted_predicted_value, formatted_error])

    result_df = pd.DataFrame(results_final, columns=['True_Value', 'Predicted_Value', 'Error'])
    result_df.to_csv("cascade_forest_predictions_memory_optimized.csv", index=False)
    print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: cascade_forest_predictions_memory_optimized.csv")

    # çº§è”æ£®æ—åˆ†æ
    print("\n" + "=" * 60)
    print("çº§è”æ£®æ—æ¶æ„åˆ†æ")
    print("=" * 60)

    for target_name in columns:
        model = cascade_models[target_name]
        if hasattr(model, 'best_layer'):
            print(f"{target_name}: ä½¿ç”¨ {model.best_layer + 1} å±‚çº§è” (æ€»å…±è®­ç»ƒäº† {len(model.layers)} å±‚)")
        else:
            print(f"{target_name}: ä½¿ç”¨åå¤‡æ¨¡å‹")

    end_time = time.time()
    total_time = end_time - start_time
    print(f"\næ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time / 60:.2f} åˆ†é’Ÿ)")

    return avg_mae, avg_r2


if __name__ == "__main__":
    print("å¼€å§‹çº§è”æ£®æ—è®­ç»ƒä¸é¢„æµ‹ (æ”¯æŒå¤šå±‚æ¶æ„)...")
    avg_mae, avg_r2 = cascade_forest_predict()
    print(f"\nğŸ‰ çº§è”æ£®æ—é¢„æµ‹å®Œæˆ!")
    print(f"å¹³å‡MAE: {avg_mae:.4f}")
    print(f"å¹³å‡RÂ²: {avg_r2:.4f}")